# -*- coding: utf-8 -*-
"""forward_Back_Propagation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FkHs1SWMEJ_MaiQAbwUoWHpTtqXS74zT
"""

#why I put forward_propagation and back propagation in one function
#Because back propagation will use the value in forward propagation 
import numpy as np
import linear_Forward as lF
import sigmoid

def forward_Back_Propagation(X,W1,B1,W2,B2,Y):
  #this function computes forward propagation and back propagation
  '''
  Arguements:
  X -- data of size(75000,number_of_examples)
  W1 -- weights of first layer of size (75000,10)
  W2 -- weights of second layer of size (10,1)
  B1 -- bias units of size(10,1)
  B2 -- bias units of size(1,1)
  Y -- data of size(1,number_of_examples),true labels vector

  Returns:
  costs -- costs for logistic regression
  dW -- gradients of W
  dB -- gradients of B
  '''

  #get the constant that will use
  m = X.shape[1]

  #forward propagation
  Z1 = np.dot(W1.T,X) + B1
  A1 = sigmoid(Z1)
  Z2 = np.dot(W2.T,A1) + B2
  A2 = sigmoid(Z2)
  costs = -sum(sum(Y*np.log(A2)+(1-Y)*np.log(1-A2)))/m

  #back propagation
  dZ2 = Y - A2
  dW2 = np.dot(A1,dZ2.T)
  dB2 = dZ2
  dZ1 = np.dot(W2,dZ2)*A1*(1-A1)
  dW1 = np.dot(X,dZ1.T) #关于这里要不要 /m 仍然是个疑思，还需要进一步考虑一下
  dB1 = np.dot(W2,dZ2)*A1

  grad1 = {'dW1':dW1,
           'dB1':dB1}

  grad2 = {'dW2':dW2,
           'dB1':dB1}
  
  return grad1,grad2,costs