{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"linearForward.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKnGVPtb2ymCyNvmWHCvjT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"gILT6W91_spg","executionInfo":{"status":"ok","timestamp":1620462532295,"user_tz":-480,"elapsed":1539,"user":{"displayName":"Lorange Loone","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQhzwRx8uAokUlJSdgj5pvMsTsL0sIlRYZlye8=s64","userId":"10114583141207595884"}}},"source":["import numpy as np\n","\n","def linearForward(w,a,b):\n","  ''' compute linear forward of z = w*a + b\n","\n","    Arguments:\n","    w, weight of layer l ,a vector, (nl,1);\n","    a, the activation of layer l-1, a vector, (nl-1,1);\n","    b, the bias term of layer l, a vector, (nl,1)\n","\n","    Returns:\n","    z , the linear forward result, a vector or a scalar, (nl,1)\n","    '''\n","  z = np.dot(w,a) + b;\n","  return z"],"execution_count":11,"outputs":[]}]}